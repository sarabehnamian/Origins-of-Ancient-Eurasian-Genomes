{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBX_r7jFTLq7"
      },
      "source": [
        "## Summary: \n",
        "\n",
        " We estimate the age (DateBP) of the genomes in this notebook.\n",
        "\n",
        "## Solution Steps: \n",
        "1. Accessing the data\n",
        "\n",
        "2. Exploring the data (initial data analysis)\n",
        "\n",
        "  2.1. Exploring feature and target distribution\n",
        "  \n",
        "3. Preparing the data\n",
        "\n",
        "  3.1. Cleaning the data\n",
        "\n",
        "  3.2. Filtering important features\n",
        "\n",
        "  3.3. Checking missing values\n",
        "\n",
        "  3.4. Checking for correlation and important features\n",
        "\n",
        "  3.5. Feature engineering (feature extraction, feature selection)\n",
        "\n",
        "4. Training model\n",
        "\n",
        "5. Using the model (Prediction)\n",
        "\n",
        "6. Evaluating the model (accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AccqhzGZTJND"
      },
      "source": [
        "## **Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mGn9F5FyOshY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.stats import zscore,norm\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import display\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "import scipy.stats\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAUwKmgzQfrv"
      },
      "source": [
        "## **Accessing the data** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgWzQIHRhFHB"
      },
      "source": [
        "We have two main datasets, **Ancients** and **Moderns**.\n",
        "\n",
        " Ancient genomes from Europe and Asia are found in Ancients, while Euroasian genomes with a DataBP of 10 are found in Moderns.\n",
        "\n",
        "**Note:** \n",
        "Dataset files should be available on data folder (Euroasian.xlsx, Euroasian_modern_samples.csv, 20130606_sample_info.xlsx, Test Cases.xlsxm Reich dataset V50.xlsx) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YFw0ajadIr6F"
      },
      "outputs": [],
      "source": [
        "baseDir='data'\n",
        "#ancient samples\n",
        "dataset =  pd.ExcelFile(f\"{baseDir}/Euroasian - Dataset_tims.xlsx\")\n",
        "Euroasian = pd.read_excel(dataset, 'Euroasian')\n",
        "Ancients = Euroasian[[ 'AncientComponent1', 'AncientComponent2','AncientComponent3', 'AncientComponent4', 'AncientComponent5', 'ModernComponent1',\n",
        "                'ModernComponent2', 'ModernComponent3', 'Date mean in BP in years before 1950 CE [OxCal mu for a direct radiocarbon date, and average of range for a contextual date]'\n",
        "                ,'Method for Determining Date; unless otherwise specified, calibrations use 95.4% intervals from OxCal v4.4.2 Bronk Ramsey (2009); r:5; Atmospheric data from Reimer et al (2020)'\n",
        "                , 'Sample ID', 'Country'\n",
        "                ,'Date standard deviation in BP [OxCal sigma for a direct radiocarbon date, and standard deviation of the uniform disribution between the two bounds for a contextual date]'              \n",
        "]]\n",
        "\n",
        "Ancients.rename(columns={\n",
        "    'Method for Determining Date; unless otherwise specified, calibrations use 95.4% intervals from OxCal v4.4.2 Bronk Ramsey (2009); r:5; Atmospheric data from Reimer et al (2020)':'Dating',\n",
        "    'Date standard deviation in BP [OxCal sigma for a direct radiocarbon date, and standard deviation of the uniform disribution between the two bounds for a contextual date]':'STD',\n",
        "    'Date mean in BP in years before 1950 CE [OxCal mu for a direct radiocarbon date, and average of range for a contextual date]':'Mean date (BP)'\n",
        "},inplace=True)\n",
        "\n",
        "# modern samples (DateBP = 10)\n",
        "modern = pd.read_csv(f\"{baseDir}/Euroasian_modern_samples.csv\")\n",
        "modern = modern[[ 'AncientComponent1', 'AncientComponent2','AncientComponent3','AncientComponent4', 'AncientComponent5', 'ModernComponent1',\n",
        "                                'ModernComponent2','ModernComponent3', 'DateBP', 'Sample ID']] \n",
        "modern.rename(columns = {'DateBP':'Mean date (BP)'}, inplace=True) \n",
        "\n",
        "#modern samples' annotation\n",
        "modern_annotation = pd.read_excel(f\"{baseDir}/20130606_sample_info.xlsx\")\n",
        "modern_annotation.rename(columns = {'Sample':'Sample ID'}, inplace=True) \n",
        "modern_annotation.rename(columns = {'Population':'Country'}, inplace=True) \n",
        "Moderns = pd.merge(modern_annotation, modern, on=[\"Sample ID\"])\n",
        "Moderns['Dating'] = np.nan\n",
        "Moderns = Moderns[['AncientComponent1', 'AncientComponent2','AncientComponent3', 'AncientComponent4', 'AncientComponent5', 'ModernComponent1',\n",
        "                'ModernComponent2', 'ModernComponent3', 'Mean date (BP)', 'Dating', 'Sample ID', 'Country']]\n",
        "\n",
        "#test_cases\n",
        "test_cases = pd.ExcelFile(f\"{baseDir}/Test Cases.xlsx\")\n",
        "Brandysek_inds = pd.read_excel(test_cases, 'Brandysek inds')\n",
        "Brandysek_inds = Brandysek_inds.iloc[0:14,:]\n",
        "Brandysek_inds.rename(columns={'Version ID':'Sample ID'},inplace=True)\n",
        "\n",
        "riech = pd.read_excel(f\"{baseDir}/Reich dataset V50.xlsx\")\n",
        "riech.rename(columns = {'Version ID':'Sample ID'}, inplace=True) \n",
        "\n",
        "Moderns['STD'] = 0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTlJOMtWUXe"
      },
      "source": [
        "## **Exploring the data (statistical initial analysis)** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hA70RMGbWCa0"
      },
      "outputs": [],
      "source": [
        "print(Ancients.info())\n",
        "display(Ancients.head(3))\n",
        "print(Moderns.info())\n",
        "display(Moderns.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-BfIbM4by2L6"
      },
      "outputs": [],
      "source": [
        "sns.distplot(Ancients['Mean date (BP)'], fit = norm) # Plot the distribution with a histogram and maximum likelihood gaussian distribution fit.\n",
        "fig = plt.figure()\n",
        "stats.probplot(Ancients['Mean date (BP)'], plot=plt) # Generates a probability plot of sample data against the quantiles of a specified theoretical distribution (the normal distribution by default).\n",
        "\"\"\"Skewness is a measure of symmetry, or more precisely, the lack of symmetry.\n",
        " Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution.\n",
        " That is, data sets with high kurtosis tend to have heavy tails, or outliers.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxSwdV_DFB0d"
      },
      "source": [
        "## **Cleaning the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QC8Wl_usdt1p"
      },
      "outputs": [],
      "source": [
        "def outlier_by_zscore(df:pd.DataFrame,column,outliers):\n",
        "    df['is_outlier'] = zscore(df[column])\n",
        "    is_outlier = df['is_outlier'].apply(lambda x: x <= -3 or x >= 3)\n",
        "    df.drop('is_outlier',axis=1,inplace=True)\n",
        "    return df[~is_outlier],outliers.append(df[is_outlier])\n",
        "\n",
        "def preprocess(df:pd.DataFrame,outliers):\n",
        "    df,outliers = outlier_by_zscore(df,'Mean date (BP)',outliers)\n",
        "    df,outliers = outlier_by_zscore(df,'AncientComponent1',outliers)  \n",
        "    df,outliers = outlier_by_zscore(df,'AncientComponent2',outliers)\n",
        "    df,outliers = outlier_by_zscore(df,'AncientComponent3',outliers) \n",
        "    df,outliers = outlier_by_zscore(df,'AncientComponent4',outliers)\n",
        "    df,outliers = outlier_by_zscore(df,'AncientComponent5',outliers)\n",
        "    df,outliers = outlier_by_zscore(df,'ModernComponent1',outliers)\n",
        "    df,outliers = outlier_by_zscore(df,'ModernComponent2',outliers)\n",
        "    df,outliers = outlier_by_zscore(df,'ModernComponent3',outliers)\n",
        "\n",
        "    outliers11 = df[df.STD >= 400]\n",
        "    outliers = outliers.append(outliers11)\n",
        "    df = df[df.STD <400]\n",
        "    return df,outliers\n",
        "outliers:pd.DataFrame = pd.DataFrame()\n",
        "Ancients,outliers = preprocess(Ancients,outliers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYyRpF0haVXT"
      },
      "source": [
        "## **Checking the missing values** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZDNN1MgKaXJ6"
      },
      "outputs": [],
      "source": [
        "print(Ancients.isnull().sum()) # No missing values are found\n",
        "print(Moderns1.isnull().sum()) # No missing values are found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c9JuJ9NxCrzx"
      },
      "outputs": [],
      "source": [
        "sns.distplot(Ancients['Mean date (BP)'], fit = norm)\n",
        "fig = plt.figure()\n",
        "stats.probplot(Ancients['Mean date (BP)'], plot = plt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zPNi0eVbVpu"
      },
      "source": [
        "## **Checking for correlation and important features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IYIosErVaK0Y"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(Ancients.iloc[:,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tXhKSXAlZ2t3"
      },
      "outputs": [],
      "source": [
        "corr = Ancients.corr().round(2)\n",
        "sns.heatmap(data = corr, annot=True)\n",
        "plt.show() #the features are not highly correlated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqy87DHLbBbt"
      },
      "source": [
        "##**Feature engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hFc0EOYbOxPD"
      },
      "outputs": [],
      "source": [
        "#creating new columns based on the values of existing ones\n",
        "Ancients['mean_Ancient_columns'] = Ancients.loc[:,['AncientComponent1',\t'AncientComponent2',\t'AncientComponent3',\t'AncientComponent4',\t'AncientComponent5']].mean(axis=1)  \n",
        "AncientComponent1_mean = Ancients['AncientComponent1'].mean()\n",
        "Ancients['AncientComponent1_diff_mean'] = abs(Ancients['AncientComponent1'] - AncientComponent1_mean)  \n",
        "Ancients['AncientComponent1'] = Ancients['AncientComponent1']+(3*Ancients['AncientComponent1_diff_mean'])\n",
        "\n",
        "Moderns['mean_Ancient_columns'] = Moderns.loc[:,['AncientComponent1',\t'AncientComponent2',\t'AncientComponent3',\t'AncientComponent4',\t'AncientComponent5']].mean(axis=1)     \n",
        "Moderns['AncientComponent1_diff_mean'] = abs(Moderns['AncientComponent1'] - AncientComponent1_mean)\n",
        "Moderns['AncientComponent1'] = Moderns['AncientComponent1']+(3*Moderns['AncientComponent1_diff_mean'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sim7UXYXsUZ5"
      },
      "source": [
        "## **Splitting the data**\n",
        "\n",
        " ## **Train_set, Unseen_data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc_Ya8d-9STH",
        "outputId": "888aa729-547b-4404-9289-79ce64b0f43d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data for Modeling: (4157, 11)\n",
            "Unseen Data For Predictions(741, 11)\n"
          ]
        }
      ],
      "source": [
        "#removing test cases IDs\n",
        "Brandysek_inds_in_Ancients = Ancients[Ancients[\"Sample ID\"].isin(Brandysek_inds[\"Sample ID\"].values)]\n",
        "Ancients = Ancients.drop(Brandysek_inds_in_Ancients.index)\n",
        "\n",
        "# In order to get a better result, we stratified the dataset, so we need to add a few more columns.\n",
        "# creating temporary columns\n",
        "Ancients['cut'] = pd.qcut(Ancients['Mean date (BP)'], q = 10, duplicates= 'drop',labels=False, precision = 0) #df was divided into 10 subsets based on DateBP column.\n",
        "Ancients['Country_cut'] = Ancients['cut'].astype(str) + Ancients['Country'].astype(str)\n",
        "count_freq = dict(Ancients['Country_cut'].value_counts())\n",
        "Ancients['count_freq'] = Ancients['Country_cut']\n",
        "Ancients['count_freq'] = Ancients['count_freq'].map(count_freq)\n",
        "train1 = Ancients[Ancients.count_freq == 1] #shape =((63,10), 63 of them  were repeated once; we keep them for training.\n",
        "Ancients  = Ancients[Ancients.count_freq >1]\n",
        "\n",
        "#stratified ancient genomes based on location and age\n",
        "train_set, test_set = train_test_split(Ancients, test_size = 0.15, random_state = 0, stratify = Ancients['Country_cut']) \n",
        "train_set = train_set.append([train1])\n",
        "\n",
        "# removing temporary columns\n",
        "train_set = train_set[train_set.columns.difference(['Country_cut', 'count_freq', 'cut'])]\n",
        "test_set = test_set[test_set.columns.difference(['Country_cut', 'count_freq', 'cut'])]\n",
        "\n",
        "#stratified modern samples based on location\n",
        "Moderns['location'] = Moderns['Country'].astype(str)  #{'CDX': 93,'CEU': 99, 'CHB': 103, 'CHS': 105, 'FIN': 99,'GBR': 91,'IBS': 107,'ITU': 102, 'JPT': 104,'KHV': 99,'PJL': 96,\n",
        "#'STU': 102,'TSI': 107}\n",
        "modern_for_modeling, modern_for_test = train_test_split(Moderns, test_size = 0.15, random_state = 42, stratify = Moderns['location']) \n",
        "modern_for_test = modern_for_test[modern_for_test.columns.difference([ 'location'])]\n",
        "modern_for_modeling = modern_for_modeling[modern_for_modeling.columns.difference([ 'location'])]\n",
        "\n",
        "#unseen data, train_set, test_set (validation)\n",
        "test_set = test_set.append([modern_for_test, Brandysek_inds_in_Ancients])\n",
        "test_set.sample(frac=1) # shuffle the DataFrame rows\n",
        "train_set = train_set.append([modern_for_modeling])\n",
        "train_set =train_set.sample(frac=1)\n",
        "\n",
        "#dropping Sample ID and Counrty columns    \n",
        "whole_set = train_set.append([test_set])\n",
        "whole_set1 = whole_set.copy()\n",
        "whole_set = whole_set[whole_set.columns.difference(['Sample ID','Dating', 'Country', 'STD'])]\n",
        "train_set1 = train_set.copy()\n",
        "train_set = train_set[train_set.columns.difference(['Sample ID', 'Dating', 'Country','STD'])]\n",
        "test_set1 = test_set.copy()\n",
        "test_set = test_set[test_set.columns.difference(['Sample ID', 'Dating', 'Country', 'STD'])]\n",
        "\n",
        "X_train = train_set[train_set.columns.difference(['Mean date (BP)'])]\n",
        "y_train = train_set['Mean date (BP)']\n",
        "X_test = test_set[test_set.columns.difference(['Mean date (BP)'])]\n",
        "y_test = test_set['Mean date (BP)']\n",
        "X_whole = whole_set[whole_set.columns.difference(['Mean date (BP)'])]\n",
        "y_whole = whole_set['Mean date (BP)']\n",
        "\n",
        "print('Data for Modeling: ' + str(train_set.shape))\n",
        "print('Unseen Data For Predictions' + str(test_set.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEcty25_emNA"
      },
      "source": [
        " ## **Training  model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "waPyHSJ9XVuJ"
      },
      "outputs": [],
      "source": [
        "def cross_validation(df,target,k=10):\n",
        "  X=df.drop(target,axis=1).values\n",
        "  y=df[target].values\n",
        "  kf = KFold(n_splits=k,shuffle=True,random_state=32)\n",
        "  R2= []\n",
        "  model = []\n",
        "  nth_k=1\n",
        "  nth_K_X_train={}\n",
        "  rf_reg = RandomForestRegressor()\n",
        "  for train_index, val_index in kf.split(X):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "    nth_K_X_train[nth_k]=X_train\n",
        "    rf_reg.fit(X_train, y_train)\n",
        "    y_val_pred = rf_reg.predict(X_val)\n",
        "    this_k_R2 = rf_reg.score(X_val,y_val)\n",
        "    R2.append(this_k_R2)\n",
        "    model.append(rf_reg)\n",
        "    nth_k+=1 \n",
        "  return model,R2,nth_K_X_train \n",
        "\n",
        "model,R2,nth_K_X_train = cross_validation(df=train_set,target='Mean date (BP)', k=10)\n",
        "\n",
        "k=10\n",
        "random.seed(0)\n",
        "random_fold = random.randrange(0, 10)\n",
        "random_model = model[random_fold]\n",
        "random_X_train = nth_K_X_train[random_fold]\n",
        "\n",
        "# # save the model\n",
        "# pickle.dump(random_model, open( 'random_model.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fDwQLXEO43I2"
      },
      "outputs": [],
      "source": [
        "# load the model from disk\n",
        "loaded_model = pickle.load(open('/content/drive/MyDrive/ml_models/random_model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0hUGwusXlqX"
      },
      "source": [
        " ## **Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pYJXyCPeqGGz"
      },
      "outputs": [],
      "source": [
        "test_predictions = X_test.copy()\n",
        "test_predictions['Mean date (BP)'] = y_test\n",
        "test_predictions['TPS predicted date (BP)'] = loaded_model.predict(X_test).round()\n",
        "test_predictions = pd.concat([test_set1[['Sample ID', 'Country', 'Dating']],test_predictions],axis=1)\n",
        "test_predictions_final = test_predictions\n",
        "test_predictions_final['Mean date (BP)'] = test_predictions_final['Mean date (BP)'].astype('float')\n",
        "test_predictions_final[\"difference\"] = abs(test_predictions_final['TPS predicted date (BP)']-test_predictions_final['Mean date (BP)'])\n",
        "\n",
        "Whole_predictions = X_whole.copy()\n",
        "Whole_predictions['Mean date (BP)'] = y_whole\n",
        "Whole_predictions['TPS predicted date (BP)'] = loaded_model.predict(X_whole).round()\n",
        "Whole_predictions = pd.concat([whole_set1[['Sample ID', 'Country', 'Dating']],Whole_predictions],axis=1)\n",
        "Whole_predictions_final = Whole_predictions\n",
        "Whole_predictions_final['Mean date (BP)'] = Whole_predictions_final['Mean date (BP)'].astype('float')\n",
        "Whole_predictions_final[\"difference\"] = abs(Whole_predictions_final['TPS predicted date (BP)']-Whole_predictions_final['Mean date (BP)'])\n",
        "\n",
        "X_Brandysek = Brandysek_inds_in_Ancients[Brandysek_inds_in_Ancients.columns.difference(['Sample ID', 'Dating', 'Country', 'Mean date (BP)', 'STD'])]\n",
        "y_Brandysek = Brandysek_inds_in_Ancients['Mean date (BP)']\n",
        "Brandysek_inds_in_Ancients1 = Brandysek_inds_in_Ancients.copy()\n",
        "Brandysek_inds_in_Ancients_predictions = X_Brandysek.copy()\n",
        "Brandysek_inds_in_Ancients_predictions['Mean date (BP)'] = y_Brandysek\n",
        "Brandysek_inds_in_Ancients_predictions['TPS predicted date (BP)'] = loaded_model.predict(X_Brandysek).round()\n",
        "Brandysek_inds_in_Ancients_predictions = pd.concat([Brandysek_inds_in_Ancients1[['Sample ID', 'Country', 'Dating']],Brandysek_inds_in_Ancients_predictions],axis=1)\n",
        "Brandysek_inds_in_Ancients_predictions[\"difference\"] = abs(Brandysek_inds_in_Ancients_predictions['TPS predicted date (BP)']-Brandysek_inds_in_Ancients_predictions['Mean date (BP)'])\n",
        "Brandysek_inds_in_Ancients_predictions = Brandysek_inds_in_Ancients_predictions[[ 'Sample ID','AncientComponent1', 'AncientComponent2','AncientComponent3', 'AncientComponent4', 'AncientComponent5', 'ModernComponent1',\n",
        "                'ModernComponent2', 'ModernComponent3', 'Country', 'Dating', 'Mean date (BP)','TPS predicted date (BP)','difference']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f75vWbJJXsx-"
      },
      "source": [
        "## **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWKnUDJyFyo2",
        "outputId": "ebffbffa-e893-4840-a828-d33cbf46e259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set evaluation:\n",
            "_____________________________________\n",
            "MAE: 462.3314035087719\n",
            "MSE: 596298.8754708503\n",
            "RMSE: 772.2039079613947\n",
            "R2 Square 0.8842938730205804\n",
            "__________________________________\n",
            "Train set evaluation:\n",
            "_____________________________________\n",
            "MAE: 226.7445898484484\n",
            "MSE: 164492.75260096224\n",
            "RMSE: 405.57706123616293\n",
            "R2 Square 0.968424198595618\n",
            "__________________________________\n"
          ]
        }
      ],
      "source": [
        "def evaluate(y, yhat, model, X):\n",
        "    mae = metrics.mean_absolute_error(y, yhat)\n",
        "    mse = metrics.mean_squared_error(y, yhat)\n",
        "    rmse = np.sqrt(metrics.mean_squared_error(y, yhat))\n",
        "    r2_square = model.score(X, y)\n",
        "    print('MAE:', mae)\n",
        "    print('MSE:', mse)\n",
        "    print('RMSE:', rmse)\n",
        "    print('R2 Square', r2_square)\n",
        "    print('__________________________________')\n",
        "\n",
        "test_pred = loaded_model.predict(X_test)\n",
        "train_pred = loaded_model.predict(X_train)\n",
        "\n",
        "print('Test set evaluation:\\n_____________________________________')\n",
        "evaluate(y_test, test_pred, loaded_model, X_test)\n",
        "\n",
        "print('Train set evaluation:\\n_____________________________________')\n",
        "evaluate(y_train, train_pred, loaded_model, X_train)    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GitHub.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
